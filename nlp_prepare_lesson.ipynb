{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e686dba8",
   "metadata": {},
   "source": [
    "# Parsing Text (aka Prepping Text Data)\n",
    "In this lesson we'll take our acquired data and parse it, that is, we'll better understand the text data by *breaking it down into smaller components.\n",
    "\n",
    "We'll be using the `nltk` package, which requires a little bit of up-front setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6790bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need to install nltk, it should come with anaconda, but nltk needs to download data.\n",
    "# python -c \"import nltk; nltk.download('stopwords')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514d82d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from time import strftime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79403204",
   "metadata": {},
   "source": [
    "Here's our plan for parsing the text data:\n",
    "\n",
    "1. Convert text to all lower case for normalcy.\n",
    "1. Remove any accented characters, non-ASCII characters.\n",
    "1. Remove special characters.\n",
    "1. Tokenize the words.\n",
    "1. Stem or lemmatize the words.\n",
    "1. Remove stopwords.\n",
    "1. Store the clean text and the original text for use in future notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d89b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = '''\n",
    "What are the Math and Stats Principles You Need for Data Science?Oct 21, 2020 | Data Science\n",
    "\n",
    "\n",
    "Coming into our Data Science program, you will need to know some math and stats. However, many of our applicants actually learn in the application process – you don’t need to be an expert before applying! Data science is a very accessible field to anyone dedicated to learning new skills, and we can work with any applicant to help them learn what they need to know. But what “skills” do we mean, exactly? Just what exactly are the data science math and stats principles you need to know?\n",
    "What are the main math principles you need to know to get into Codeup’s Data Science program?\n",
    "\n",
    "\n",
    "Algebra\n",
    "Do you know PEMDAS and can you solve for x? You will need to be or become comfortable with the following:\n",
    "\n",
    "Variables (x, y, n, etc.)\n",
    "Formulas, functions, and variable manipulations (e.g. x^2 = x + 6, solve for x).\n",
    "Order of evaluation: PEMDAS: parentheses, exponents, then multiplication, division, addition, and subtraction\n",
    "Commutativity where a + b = b + a\n",
    "Associativity where a + (b + c) = (a + b) + c\n",
    "Adding and subtracting matrices\n",
    "A conceptual understanding of exponential growth/decay- things can increase at an increasing rate\n",
    "\n",
    "Descriptive Statistics\n",
    "Know what a min, max, mode, median, and average are. Have a conceptual understanding that stats/probability is about trying to quantify uncertainty.\n",
    "Data Visualization\n",
    "Know what a scatterplot is and how to read a barplot.\n",
    "How to Learn and Expand on These Concepts\n",
    "There are a number of great resources out there to teach you these and similar concepts. Khan Academy is a great starting place for data science math! If you want to know what exactly we assign our applicants, you’ll just have to apply!\n",
    " \n",
    "What about once you’re in Codeup?\n",
    "\n",
    "\n",
    "What You Won’t Do\n",
    "Do we do any mathematical proofs for concepts or perform derivations? No. \n",
    "Do we do any calculus and probability calculating by hand? No.\n",
    "Are we transforming equations, where we cancel out units or terms and do lots of algebraic gymnastics? No\n",
    "What You Will Do\n",
    "Will we have Python solve our linear algebra problems for us? Yes\n",
    "Will we have Python calculate probabilities, the area under a curve, and the slope of a line for us? Yes\n",
    "Will we have Python do all of the calculus for us? Yes\n",
    " \n",
    "See, the data science math and stats slice of the pie is certainly doable. If you like problem-solving and are ready to challenge yourself, you’ll love data science! If you are interested in learning about data science, just apply! Our Admissions Manager can work with you to get you where you need to be starting from where you are now. Let us help you get there so you can launch a great new career.\n",
    "\n",
    "Request More Info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Our ProgramsFull Stack Web Development\n",
    "Data Science\n",
    "Cyber Cloud\n",
    "Systems Engineering\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Latest Blog Articles\n",
    "Codeup Dallas Open House\n",
    "Codeup’s Placement Team Continues Setting Records\n",
    "IT Certifications 101: Why They Matter, and Why They Don’t\n",
    "A rise in cyber attacks means opportunities for veterans in San Antonio\n",
    "Use your GI Bill® benefits to Land a Job in Tech\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "More From This Category\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Codeup Dallas Open House\n",
    "Nov 30, 2021 | Dallas Newsletter, EventsCome join us for the re-opening of our Dallas Campus with some drinks and snacks at Codeup! Curious about what our campus looks like? Click here to register for free About this event Come join us for the re-opening of our Dallas Campus with some drinks and snacks at...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Codeup’s Placement Team Continues Setting Records\n",
    "Nov 19, 2021 | Codeup News, EmployersOur Placement Team is simply defined as a group that manages relationships with our employer partners and our graduating students to help get our graduating students hired. Last quarter the Placement Team helped 48 students get hired to life-changing careers in tech....\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "IT Certifications 101: Why They Matter, and Why They Don’t\n",
    "Nov 18, 2021 | Cybersecurity, IT Training, Tips for Prospective StudentsAWS, Google, Azure, Red Hat, CompTIA...these are big names in IT! And not only for their products, but also for the certifications they offer. If you’re new to tech, you might be wondering: Do certifications really matter? Welcome to IT Certifications 101! What’s the...\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af060646",
   "metadata": {},
   "source": [
    "#### Step 1: Convert all text to lower for normalcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cfe192",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = original.lower()\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8e3823",
   "metadata": {},
   "source": [
    "#### Step 2: Remove any accented characters, non-ASCII characters.\n",
    "\n",
    "Usually in any text corpus, you might be dealing with accented characters/letters, especially if you only want to analyze the English language. Hence, we need to make sure that these characters are converted and standardized into ASCII characters. A simple example is converting é to e.\n",
    "\n",
    "We'll go about this in three steps:\n",
    "\n",
    "1. `unicodedata.normalize` removes any inconsistencies in unicode character encoding.\n",
    "1. `.encode` to convert the resulting string to the ASCII character set. We'll ignore any errors in conversion, meaning we'll drop anything that isn't an ASCII character.\n",
    "1. `.decode` to turn the resulting bytes object back into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c3ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = unicodedata.normalize('NFKD', article)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e496ecea",
   "metadata": {},
   "source": [
    "#### Step 3: Remove special characters\n",
    "\n",
    "Special characters and symbols are usually non-alphanumeric characters or even occasionally numeric characters (depending on the problem), which add to the extra noise in unstructured text. Usually, simple regular expressions (regexes) can be used to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e886940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove anything that is not a through z, a number, a single quote, or whitespace\n",
    "article = re.sub(r\"[^a-z0-9'\\s]\", '', article)\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d33fac4",
   "metadata": {},
   "source": [
    "#### Step 4: Tokenization\n",
    "\n",
    "After removing non-ASCII characters and special characters, it's common to tokenize the strings, to break words and any punctuation left over into discrete units. Tokenization is the process of breaking something down into discrete units. In the context of NLP, this means breaking text down into discrete words, punctuation, etc.\n",
    "\n",
    "We will use `nltk` to do tokenization for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e4d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(original, return_str=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d89f6a4",
   "metadata": {},
   "source": [
    "#### Step 5: Stemming and Lemmatization\n",
    "\n",
    "Usually you will want to use lemmatization. We will demonstrate why that is the case by looking at both here.\n",
    "\n",
    "#### Stemming:\n",
    "Word stems are the base form of a word.\n",
    "\n",
    "We create new words by attaching affixes in a process known as inflection. For example, \"calls\", \"called\", and \"calling\" all share the base stem \"call\".\n",
    "\n",
    "The Porter stemmer is based on the algorithm developed by its inventor, Dr. Martin Porter. Originally, the algorithm is said to have had a total of five different phases for reduction of inflections to their stems, where each phase has its own set of rules.\n",
    "\n",
    "Note that usually stemming has a fixed set of rules, hence, the root stems may not be lexicographically correct. This means that the stemmed words may not be semantically correct, and might have a chance of not being present in the dictionary (as we'll see in the output of stemming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02349d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the nltk stemmer object, then use it\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "\n",
    "ps.stem('call'), ps.stem('called'), ps.stem('calling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0849fe",
   "metadata": {},
   "source": [
    "Now we can apply this stemming transformation to all the words in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1abc48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = [ps.stem(word) for word in article.split()]\n",
    "article_stemmed = ' '.join(stems)\n",
    "print(article_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6383f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(stems).value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b51168",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "Lemmatization is very similar to stemming, however, the base form in this case is known as the root word, but not the root stem. The difference is that the root word is always a lexicographically correct word (present in the dictionary), but the root stem may not be so. Thus, root word, also known as the lemma, will always be present in the dictionary.\n",
    "\n",
    "Note that the lemmatization process is considerably slower than stemming, because an additional step is involved where the root form or lemma is formed by removing the affix from the word if and only if the lemma is present in the dictionary.\n",
    "\n",
    "Let's take a look at a simple example of the difference between stemming and lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd0f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 -c \"import nltk; nltk.download('all')\"\n",
    "# OR\n",
    "# python3 -c \"import nltk; nltk.download('wordnet')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc277fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "for word in 'study studies'.split():\n",
    "    print('stem:', ps.stem(word), '-- lemma:', wnl.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b065a5c3",
   "metadata": {},
   "source": [
    "And now we can apply lemmatization to our entire document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7112e41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [wnl.lemmatize(word) for word in article.split()]\n",
    "article_lemmatized = ' '.join(lemmas)\n",
    "\n",
    "print(article_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac3df52",
   "metadata": {},
   "source": [
    "Now that we have a list of the lemmas, we can take a look at the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee698345",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(lemmas).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98721359",
   "metadata": {},
   "source": [
    "#### Step 6: Removing stopwords\n",
    "\n",
    "Words which have little or no significance, especially when constructing meaningful features from text, are known as stop words (or stopwords). These are usually words that end up having the maximum frequency if you do a simple term or word frequency in a corpus. Typically, these can be articles, conjunctions, prepositions and so on. Some examples of stopwords: a, an, the, and like.\n",
    "\n",
    "While there is no universal stopword list, we will use a standard English language stopwords list from nltk. You can also add your own domain-specific stopwords as needed.\n",
    "\n",
    "Before removing stopwords, we want to segment text into linguistic units such as words or numbers. This process is called tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf65e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words('english')\n",
    "\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "\n",
    "stopword_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e81f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = article.split()\n",
    "filtered_words = [w for w in words if w not in stopword_list]\n",
    "\n",
    "print('Removed {} stopwords'.format(len(words) - len(filtered_words)))\n",
    "print('---')\n",
    "\n",
    "article_without_stopwords = ' '.join(filtered_words)\n",
    "\n",
    "print(article_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81315ea3",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "The end result of this exercise should be a file named `prepare.py` that defines the requested functions.\n",
    "\n",
    "In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired.\n",
    "\n",
    "1. Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "- Lowercase everything\n",
    "- Normalize unicode characters\n",
    "- Replace anything that is not a letter, number, whitespace or a single quote.\n",
    "\n",
    "2. Define a function named `tokenize`. It should take in a string and tokenize all the words in the string.\n",
    "\n",
    "3. Define a function named `stem`. It should accept some text and return the text after applying stemming to all the words.\n",
    "\n",
    "4. Define a function named `lemmatize`. It should accept some text and return the text after applying lemmatization to each word.\n",
    "\n",
    "5. Define a function named `remove_stopwords`. It should accept some text and return the text after removing all the stopwords.\n",
    "\n",
    "    This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove.\n",
    "    \n",
    "\n",
    "6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe `news_df`.\n",
    "\n",
    "7. Make another dataframe for the Codeup blog posts. Name the dataframe `codeup_df`.\n",
    "\n",
    "8. For each dataframe, produce the following columns:\n",
    "- `title` to hold the title\n",
    "- `original` to hold the original article/post content\n",
    "- `clean` to hold the normalized and tokenized original with the stopwords removed.\n",
    "- `stemmed` to hold the stemmed version of the cleaned data.\n",
    "- `lemmatized` to hold the lemmatized version of the cleaned data.\n",
    "\n",
    "Ask yourself:\n",
    "\n",
    "If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

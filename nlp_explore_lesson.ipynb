{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP EDA\n",
    "\n",
    "Basically, exploration and modeling boil down to a single question:\n",
    "\n",
    "How do we quantify our data/text\n",
    "\n",
    "In this lesson, we'll explore answers to this question that will aid in visualization.\n",
    "\n",
    "- word frequency (by label)\n",
    "- ngrams\n",
    "- word cloud\n",
    "- sentiment analysis\n",
    "- other common features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Data is spam/ham text messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('figure', figsize=(13, 7))\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    '''Simplified text cleaning function'''\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return re.sub(r\"[^a-z0-9\\s]\", '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acquire data from spam_db\n",
    "\n",
    "from env import user, password, host\n",
    "\n",
    "def get_db_url(database, host=host, user=user, password=password):\n",
    "    return f'mysql+pymysql://{user}:{password}@{host}/{database}'\n",
    "\n",
    "\n",
    "url = get_db_url(\"spam_db\")\n",
    "sql = \"SELECT * FROM spam\"\n",
    "\n",
    "df = pd.read_sql(sql, url, index_col=\"id\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_colwidth', -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many spam and ham observations do we have?\n",
    "\n",
    "df.label.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all messages in single string by label\n",
    "\n",
    "ham_words = clean(' '.join(df[df.label == 'ham'].text))\n",
    "spam_words = clean(' '.join(df[df.label == 'spam'].text))\n",
    "all_words = clean(' '.join(df.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "Represent text as word frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_freq = pd.Series(ham_words.split()).value_counts()\n",
    "spam_freq = pd.Series(spam_words.split()).value_counts()\n",
    "all_freq = pd.Series(all_words.split()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# concat all frequencies together into a dataframe\n",
    "\n",
    "word_counts = pd.concat([ham_freq, spam_freq, all_freq], axis=1).fillna(0).astype(int)\n",
    "word_counts.columns = ['ham', 'spam', 'all']\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What are the most frequently occuring words?\n",
    "- Are there any words that uniquely identify a spam or ham message? I.e. words present in one type of message but not the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by 'all'\n",
    "\n",
    "word_counts.sort_values('all', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by 'ham' and 'spam' columns\n",
    "word_counts.sort_values(['ham', 'spam'], ascending=[True, False]).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "- ham vs spam count for 20 most common words\n",
    "- ham vs spam proportion for 20 most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=18)\n",
    "word_counts.sort_values('all', ascending=False).head(20)[['spam', 'ham']].plot.barh()\n",
    "plt.title('Ham vs Spam count for the top 20 most frequent words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "plt.rc('font', size=16)\n",
    "\n",
    "(word_counts.sort_values('all', ascending=False)\n",
    " .head(20)\n",
    " .apply(lambda row: row/row['all'], axis = 1)\n",
    " .drop(columns = 'all')\n",
    " .sort_values(by = 'spam')\n",
    " .plot.barh(stacked = True, width = 1, ec = 'k')\n",
    ")\n",
    "plt.title('% of spam vs ham for the most common 20 words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams\n",
    "\n",
    "**bigram**: combinations of 2 words\n",
    "\n",
    "Represent text as combinations of 2 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Today is tuesday, and the weather is nice.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nltk.bigrams(sentence.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Be Careful!** Make sure you are making bigrams out of *words*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't do this!\n",
    "# list(nltk.bigrams(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- what are the most common bigrams? spam bigrams? ham bigrams?\n",
    "- visualize 20 most common bigrams, most common ham bigrams\n",
    "- ngrams\n",
    "\n",
    "Find the most common bigram and then find a representative text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(nltk.bigrams(spam_words.split())).value_counts().head(10).plot.barh()\n",
    "plt.title('Top 10 most common spam bigrams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`python -m pip install --upgrade wordcloud`\n",
    "\n",
    "documentation: https://amueller.github.io/word_cloud/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "img = WordCloud(background_color='white', width=800, height=600).generate(spam_words)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Common Features\n",
    "\n",
    "Any NLP dataset will have domain specific features, for example: number of retweets, number of @mentions, number of upvotes, or mean time to respond to a support chat. In addition to these domain specific features, some common measures for a document are:\n",
    "\n",
    "- character count\n",
    "- word count\n",
    "- sentence count\n",
    "- stopword count\n",
    "- unique word count\n",
    "- punctuation count\n",
    "- average word length\n",
    "- average words per sentence\n",
    "- word to stopword ratio\n",
    "\n",
    "Create one or more of the above features and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add two new columns 'message_length' and 'word_count'\n",
    "\n",
    "df['message_length'] = df.text.apply(len)\n",
    "\n",
    "df['word_count'] = df.text.apply(clean).apply(str.split).apply(len)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data = df, x = 'message_length', y = 'word_count', hue = 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('label').message_length.agg(['mean', 'median'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment\n",
    "\n",
    "A number indicating whether the document is positive or negative.\n",
    "\n",
    "- knowledge-based + statistical approach\n",
    "- relies on human-labelled data\n",
    "    - combination of qualitative and quantitative methods\n",
    "    - then empirically validate\n",
    "- different models for diff domains (e.g. social media vs news)\n",
    "- for social media\n",
    "    - Afinn ([github](https://github.com/fnielsen/afinn) + [whitepaper](http://www2.imm.dtu.dk/pubdb/edoc/imm6006.pdf))\n",
    "    - Vader ([github](https://github.com/cjhutto/vaderSentiment) + [whitepaper](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf)) `nltk.sentiment.vader.SentimentIntensityAnalyzer`. Pre-trained sentiment analyzer (**V**alence **A**ware **D**ictionary and s**E**ntiment **R**easoner).)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From your terminal:\n",
    "`python -c 'import nltk;nltk.download(\"vader_lexicon\")'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.sentiment\n",
    "\n",
    "sia = nltk.sentiment.SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores('He is really good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores('He is really good!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores('He is REALLY good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores('He is very good!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores('The food is good but service is slow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores('He is good :-)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores('The food SUX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things that can influence Sentiment Score:\n",
    "1. Punctuations. Can increase the intensity\n",
    "2. Capitalization. Can increase the intensity\n",
    "3. Degree modifiers\n",
    "4. Conjunctions\n",
    "\n",
    "It can handle Emojis and slangs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply this to the text message data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['sentiment'] = df.text.apply(lambda doc: sia.polarity_scores(doc)['compound'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the mean and median values of sentiment score different for ham vs spam?\n",
    "df.groupby('label').sentiment.agg(['mean','median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot message_length vs sentiment and hue by label\n",
    "sns.relplot(data = df, x = 'message_length', y = 'sentiment', hue = 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the distribution for sentiment different for ham vs spam\n",
    "sns.kdeplot(df[df.label == 'ham'].sentiment, label = 'ham')\n",
    "sns.kdeplot(df[df.label == 'spam'].sentiment, label = 'spam');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate KDE plot for message_length vs sentiment score \n",
    "sns.kdeplot(df[df.label == 'ham'].message_length,df[df.label == 'ham'].sentiment, levels = 30, shade = True )\n",
    "sns.kdeplot(df[df.label == 'spam'].message_length,df[df.label == 'spam'].sentiment, levels = 30, shade = True, alpha = 0.5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Resources\n",
    "\n",
    "- [kaggle wikipedia movie plots](https://www.kaggle.com/jrobischon/wikipedia-movie-plots)\n",
    "    - Suggestion: narrow to top n genres that aren't unknown\n",
    "- [wikitable extractor](https://wikitable2csv.ggor.de/) (Try with, e.g. [helicopter prison escapes](https://en.wikipedia.org/wiki/List_of_helicopter_prison_escapes))\n",
    "- [Textblob library](https://textblob.readthedocs.io/en/dev/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
